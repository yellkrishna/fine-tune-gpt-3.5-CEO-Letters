{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction"
      ],
      "metadata": {
        "id": "RVX2LO1vBA9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this exercise is to identify the existence of a few dimensions in letters written by CEOs to shareholders using OpenAI’s language model. The language model is accessed using OpenAI’s API. The model is first fine-tuned using two of the three training datasets provided with the leftover dataset used as a validation set. We check the predicted dimensions for the validation dataset and compare it with the true values. Similarly, we also predict the dimensions for in-sample data and compare with their true values. The accuracy of the prediction is calculated.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uv5Wbbq2BFAF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ravK1MmtFLbF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from google.colab import drive\n",
        "from openai import OpenAI\n",
        "import json\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nGqiPXHgzzCu",
        "outputId": "3daae9a2-ff2f-4223-ba2b-07f8a21ca534"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology"
      ],
      "metadata": {
        "id": "LhYZ3ZtVFkqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Preprocess the Data:\n",
        "We are using the provided train2 and train3 datasets as the training set and the train1 dataset as the validation set. In this step, we concatenated the train2 and train3 datasets to create a single training set."
      ],
      "metadata": {
        "id": "WlZge9-KBQz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_train_data(train_files):\n",
        "  # Load data from Excel files\n",
        "  df_train = pd.concat([pd.read_excel(file) for file in train_files])\n",
        "\n",
        "  # Handling Missing Values\n",
        "  # Replace missing values with a placeholder or drop rows with missing values\n",
        "  df_train.dropna(subset=['paragraph'], inplace=True)\n",
        "\n",
        "  \"\"\"# Download stopwords corpus (if not already downloaded)\n",
        "  nltk.download('stopwords')\n",
        "  nltk.download('punkt')\"\"\"\n",
        "\n",
        "  # Preprocess the text data\n",
        "  # Lower-Case words\n",
        "  df_train['processed_paragraph'] = df_train['paragraph'].apply(lambda text: text.lower())\n",
        "\n",
        "  return df_train"
      ],
      "metadata": {
        "id": "AZLvTyZ3FjgS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = r'/content/drive/MyDrive/GRA_KU_Assessment/train_files'\n",
        "# Get a list of all files in the folder\n",
        "files_in_folder = os.listdir(folder_path)\n",
        "# Filter Excel files with specific names (train1, train2, train3, etc.)\n",
        "train_files = [os.path.join(folder_path, file) for file in files_in_folder if file.startswith('train') and file.endswith('.xlsx')]\n",
        "df_train = load_and_preprocess_train_data(train_files)"
      ],
      "metadata": {
        "id": "AFSqb9liISK6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data for Fine-tuning\n",
        "In this step, we convert our data into the format required for the OpenAI API.\n",
        "\n",
        "We convert the data from the table format (.xlsx format) provided to a Chat Completions API format that is accepted by OpenAI’s API. This format is a list of messages where each message has a role and content.\n",
        "\n",
        "In our training set, for each message in the requirem format will have three components for each datapoint (sentence) with each playing a different role. The first part of the message plays the role of *system*. This is where we give precise instructions to the model as to what we expect it to do. The next part of the message plays the role of *user* which contains portions of the letters from CEOs. This is supposed to be the input for fine-tuning process. The last part of the message plays the role of *assistant*, which is the result from which we want it to fine-tune. This is the training part."
      ],
      "metadata": {
        "id": "dnEbtMDTBqGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_for_fine_tuning(df_train):\n",
        "  # Convert the 'paragraph' column to a list\n",
        "  paragraphs = df_train['processed_paragraph'].tolist()\n",
        "\n",
        "  # Create a list to store the formatted data\n",
        "  train_data = []\n",
        "\n",
        "  # Iterate through each row in the DataFrame and create prompt-completion pairs\n",
        "  for index, row in df_train.iterrows():\n",
        "      prompt = row['processed_paragraph']\n",
        "      # Convert 'Yes' and 'No' to 1 and 0, respectively\n",
        "      completion = ','.join(['1' if row[col] == 'Yes' else '0' for col in df_train.columns[1:]])\n",
        "      train_data.append({\"prompt\": prompt, \"completion\": completion})\n",
        "\n",
        "  # Display a few examples to verify the format\n",
        "  for example in train_data[:1]:\n",
        "      print(example)\n",
        "\n",
        "  return train_data"
      ],
      "metadata": {
        "id": "zXRYP5lMF95a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_chat_completion(prompt_completion_data):\n",
        "    chat_completion_data = []\n",
        "\n",
        "    for entry in prompt_completion_data:\n",
        "        prompt = entry['prompt']\n",
        "        completion = entry['completion']\n",
        "\n",
        "        # Extracting the completion details and converting them into the desired format\n",
        "        completion_details = [f\"{key}: {'Yes' if value == '1' else 'No'}\" for key, value in zip(['Goal', 'Activity', 'Strategy', 'Plan', 'Structure', 'Innovation', 'Tactics', 'Relevance'], completion.split(','))]\n",
        "\n",
        "        # Joining the completion details into a single string\n",
        "        completion_text = ', '.join(completion_details)\n",
        "\n",
        "        # Creating the chat-completion format\n",
        "        conversation = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"Use the folowing step-by-step instructon to respond to the user inputs. Step 1 - In the user content which is taken from letters written by CEO to shareholders, you have to identify the existence of dimensions/qualities that are provided in this list given in brackets and that are seperated by commas ['Goal', 'Activity', 'Strategy', 'Plan', 'Structure', 'Innovation', 'Tactics', 'Relevance']. Step 2 - For each of these dimensions, if the dimension exists in the user prompt based on the assistant content I provide to you in the fine-tuning data, answer Yes, otherwise answer No. After step2, this is an example output whose template you must use to provide your answer - ['Goal: No, Activity: Yes, Strategy: Yes, Plan: Yes, Structure: Yes, Innovation: Yes, Tactics: No, Relevance: No']\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "                {\"role\": \"assistant\", \"content\": completion_text}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        chat_completion_data.append(conversation)\n",
        "\n",
        "    return chat_completion_data\n"
      ],
      "metadata": {
        "id": "HGwVnntB3kOA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = prepare_data_for_fine_tuning(df_train)\n",
        "train_data = convert_to_chat_completion(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EUVbpIvkInFY",
        "outputId": "584454dd-d5dd-420b-9a3c-c8d641bc5032"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': 'to our shareowners:\\nthis year, amazon became the fastest company ever to reach $100 billion in annual sales. also this year, amazon web services is reaching $10 billion in annual sales ... doing so at a pace even faster than amazon achieved that milestone', 'completion': '0,0,0,0,0,0,0,0,1,0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "x685jK79IunK",
        "outputId": "2a5a13cf-55da-4691-9cd5-65ee9594d635"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': [{'role': 'system', 'content': \"Use the folowing step-by-step instructon to respond to the user inputs. Step 1 - In the user content which is taken from letters written by CEO to\\xa0shareholders, you have to identify the existence of dimensions/qualities that are provided in this list given in brackets and that are seperated by commas ['Goal', 'Activity', 'Strategy', 'Plan', 'Structure', 'Innovation', 'Tactics', 'Relevance']. Step 2 - For each of these dimensions, if the dimension exists in the user prompt based on the assistant content I provide to you in the fine-tuning data, answer Yes, otherwise answer No. After step2, this is an example output whose template you must use to provide your answer - ['Goal: No, Activity: Yes, Strategy: Yes, Plan: Yes, Structure: Yes, Innovation: Yes, Tactics: No, Relevance: No']\"}, {'role': 'user', 'content': 'to our shareowners:\\nthis year, amazon became the fastest company ever to reach $100 billion in annual sales. also this year, amazon web services is reaching $10 billion in annual sales ... doing so at a pace even faster than amazon achieved that milestone'}, {'role': 'assistant', 'content': 'Goal: No, Activity: No, Strategy: No, Plan: No, Structure: No, Innovation: No, Tactics: No, Relevance: No'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune the model\n",
        "We invoke an OpenAI model, feed the training data and finetune it. The base model used for fine-tuning is gpt-3.5-turbo."
      ],
      "metadata": {
        "id": "tQON5n4_HNWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_model(train_data, api_key):\n",
        "  # Assuming train_data contains your prompt-completion pairs\n",
        "  # Save the train_data in JSON Lines format\n",
        "  with open(\"/content/drive/My Drive/mydata.jsonl\", \"w\") as file:\n",
        "      for example in train_data:\n",
        "          file.write(json.dumps(example) + \"\\n\")\n",
        "\n",
        "  # Initialize the OpenAI client\n",
        "  client = OpenAI(api_key= api_key)\n",
        "\n",
        "  # Upload the JSON Lines file for fine-tuning\n",
        "  try:\n",
        "    resp1 = client.files.create(\n",
        "        file=open(\"/content/drive/My Drive/mydata.jsonl\", \"rb\"),\n",
        "        purpose=\"fine-tune\"\n",
        "    )\n",
        "    print(\"File uploaded successfully.\")\n",
        "  except Exception as e:\n",
        "    print(\"File upload failed:\", e)\n",
        "    return None, None\n",
        "\n",
        "  # Create the fine-tuning job\n",
        "  try:\n",
        "    resp2 = client.fine_tuning.jobs.create(\n",
        "    training_file=resp1.id,\n",
        "    model=\"gpt-3.5-turbo\"\n",
        "    )\n",
        "    print(\"Fine-tuning job created successfully.\")\n",
        "  except Exception as e:\n",
        "    print(\"Fine-tuning job creation failed:\", e)\n",
        "    return None, None\n",
        "\n",
        "  # Check the status of the fine-tuning job\n",
        "  while True:\n",
        "    resp3 = client.fine_tuning.jobs.retrieve(resp2.id)\n",
        "    status = resp3.status\n",
        "    print(\"Fine-tuning job status:\", status)\n",
        "    if status == \"succeeded\":\n",
        "      print(\"Fine-tuning job completed successfully.\")\n",
        "      break\n",
        "    elif status == \"failed\":\n",
        "      print(\"Fine-tuning job failed:\", resp3.error)\n",
        "      break\n",
        "    elif status == \"cancelled\":\n",
        "      print(\"Fine-tuning job cancelled by user.\")\n",
        "      break\n",
        "    else:\n",
        "      print(\"Fine-tuning job in progress. Please wait...\")\n",
        "      time.sleep(60)\n",
        "\n",
        "  return resp2, client\n"
      ],
      "metadata": {
        "id": "2YStub91PVdZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = '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'\n",
        "response, client = fine_tune_model(train_data, api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wOi9ZoG7IyfV",
        "outputId": "ee312019-c2cc-4fe3-c2fa-2c54d4588d74"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File uploaded successfully.\n",
            "Fine-tuning job created successfully.\n",
            "Fine-tuning job status: validating_files\n",
            "Fine-tuning job in progress. Please wait...\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job in progress. Please wait...\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job in progress. Please wait...\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job in progress. Please wait...\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job in progress. Please wait...\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job in progress. Please wait...\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job in progress. Please wait...\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job in progress. Please wait...\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job in progress. Please wait...\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job in progress. Please wait...\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job in progress. Please wait...\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job in progress. Please wait...\n",
            "Fine-tuning job status: succeeded\n",
            "Fine-tuning job completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NLptqQH1I1u8",
        "outputId": "6f30f98c-6d33-493a-9796-d940010bfedd"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FineTuningJob(id='ftjob-lI1ZAkkx5HC4RUvwe4eWgsVh', created_at=1700426691, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-BNdO85mmDzyE4YWAlVvz9Z2A', result_files=[], status='validating_files', trained_tokens=None, training_file='file-ldgeYk7lEUivfLlnuD76kmgk', validation_file=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and preprocess the validation data\n",
        "We load the validation data and do the necessary preprocessing. This is the train1.xslx data."
      ],
      "metadata": {
        "id": "z9RbH12CHu_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_test_data(test_file):\n",
        "  # Load data from the test file\n",
        "  df_test = pd.read_excel(test_file)\n",
        "\n",
        "  # Preprocess the text data similar to the training data\n",
        "  df_test['processed_paragraph'] = df_test['paragraph'].apply(lambda text: text.lower())\n",
        "\n",
        "  return df_test"
      ],
      "metadata": {
        "id": "E2ynN6GgH0kv"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_file = r\"/content/drive/MyDrive/GRA_KU_Assessment/validation_file/train1.xlsx\"\n",
        "df_test = load_and_preprocess_test_data(test_file)"
      ],
      "metadata": {
        "id": "9jXK3BzLI8jR"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Predictions\n",
        "We now use the fine-tuned model and feed in the validation set provided. We will use the results from the validation set to assess the model’s performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "-erB24WCIADL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(df_test, fine_tuned_model_id, client):\n",
        "  # Get the sentences to test from the dataframe\n",
        "  sentences_to_test = df_test['processed_paragraph'].tolist()\n",
        "  # Initialize an empty list to store the responses\n",
        "  responses = []\n",
        "  # Set the batch size for chat completions\n",
        "  batch_size = 10\n",
        "  # Loop through the sentences in batches\n",
        "  for i in range(0, len(sentences_to_test), batch_size):\n",
        "    # Get the current batch of sentences\n",
        "    batch = sentences_to_test[i:i+batch_size]\n",
        "    # Loop through the sentences in the batch\n",
        "    for ind, sentence in enumerate(batch):\n",
        "      # Create the system and user messages for each sentence\n",
        "      messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Use the folowing step-by-step instructon to respond to the user inputs. Step 1 - In the user content which is taken from letters written by CEO to shareholders, you have to identify the existence of dimensions/qualities that are provided in this list given in brackets and that are seperated by commas ['Goal', 'Activity', 'Strategy', 'Plan', 'Structure', 'Innovation', 'Tactics', 'Relevance']. Step 2 - For each of these dimensions, if the dimension exists in the user prompt based on the assistant content I provide to you in the fine-tuning data, answer Yes, otherwise answer No. After step2, this is an example output whose template you must use to provide your answer - ['Goal: No, Activity: Yes, Strategy: Yes, Plan: Yes, Structure: Yes, Innovation: Yes, Tactics: No, Relevance: No']\"},\n",
        "        {\"role\": \"user\", \"content\": sentence}\n",
        "      ]\n",
        "      # Try to make a chat completion for the sentence\n",
        "      try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages\n",
        "        )\n",
        "        print(\"Chat completion succeeded for sentence\", ind, \" and batch \", i)\n",
        "      # Handle any errors or exceptions\n",
        "      except Exception as e:\n",
        "        print(\"Chat completion failed for sentence\", ind, \" and batch \", i, \":\", e)\n",
        "        return None\n",
        "      # Append the assistant message content to the responses list\n",
        "      responses.append(response.choices[0].message.content)\n",
        "\n",
        "  # Return the responses list\n",
        "  return responses\n"
      ],
      "metadata": {
        "id": "jrNXT29XS2Te"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model_id = \"gpt-3.5-turbo\"\n",
        "predictions = make_predictions(df_test, fine_tuned_model_id, client)"
      ],
      "metadata": {
        "id": "_NEqRmO0Kqu0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "86edffca-8054-41fe-d58b-8bea9f87c5ba"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat completion succeeded for sentence 0  and batch  0\n",
            "Chat completion succeeded for sentence 1  and batch  0\n",
            "Chat completion succeeded for sentence 2  and batch  0\n",
            "Chat completion succeeded for sentence 3  and batch  0\n",
            "Chat completion succeeded for sentence 4  and batch  0\n",
            "Chat completion succeeded for sentence 5  and batch  0\n",
            "Chat completion succeeded for sentence 6  and batch  0\n",
            "Chat completion succeeded for sentence 7  and batch  0\n",
            "Chat completion succeeded for sentence 8  and batch  0\n",
            "Chat completion succeeded for sentence 9  and batch  0\n",
            "Chat completion succeeded for sentence 0  and batch  10\n",
            "Chat completion succeeded for sentence 1  and batch  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Pf7491Xs57YC",
        "outputId": "f3f02b17-36e0-4b42-ce96-f176a2d738d4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"['Goal: No, Activity: No, Strategy: No, Plan: No, Structure: No, Innovation: No, Tactics: No, Relevance: No']\", \"['Goal: No, Activity: Yes, Strategy: Yes, Plan: Yes, Structure: Yes, Innovation: Yes, Tactics: No, Relevance: No']\", \"['Goal: No, Activity: Yes, Strategy: Yes, Plan: Yes, Structure: Yes, Innovation: No, Tactics: No, Relevance: No']\", \"['Goal: No, Activity: Yes, Strategy: Yes, Plan: No, Structure: Yes, Innovation: Yes, Tactics: No, Relevance: No']\", \"['Goal: No, Activity: Yes, Strategy: No, Plan: Yes, Structure: No, Innovation: Yes, Tactics: No, Relevance: Yes']\", \"['Goal: No, Activity: Yes, Strategy: Yes, Plan: Yes, Structure: Yes, Innovation: Yes, Tactics: No, Relevance: No']\", \"['Goal: No, Activity: Yes, Strategy: Yes, Plan: Yes, Structure: Yes, Innovation: Yes, Tactics: Yes, Relevance: No']\", \"['Goal: No, Activity: No, Strategy: Yes, Plan: Yes, Structure: No, Innovation: No, Tactics: No, Relevance: No']\", \"['Goal: No, Activity: No, Strategy: No, Plan: No, Structure: No, Innovation: No, Tactics: No, Relevance: No']\", \"['Goal: No, Activity: Yes, Strategy: Yes, Plan: No, Structure: No, Innovation: Yes, Tactics: No, Relevance: No']\", \"['Goal: No, Activity: Yes, Strategy: Yes, Plan: Yes, Structure: No, Innovation: No, Tactics: No, Relevance: No']\", \"['Goal: Yes, Activity: No, Strategy: Yes, Plan: Yes, Structure: Yes, Innovation: Yes, Tactics: No, Relevance: Yes']\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate performance of Fine-tuned Model:\n",
        "We assess the performance of the fine-tuned model by comparing the results from the predictions step to the actual values provided in the validation set (out-of-sample) as well as the training set (in-sample)."
      ],
      "metadata": {
        "id": "8mBR2-sg-4GG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score as acc_score, f1_score as f1, classification_report as class_rep\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_classification(df_test, predictions):\n",
        "    output_list = [item[1:-1].split(', ') for item in predictions]\n",
        "    processed_data = []\n",
        "\n",
        "    for row in output_list:\n",
        "        processed_row = [1 if item.split(': ')[1].strip(\"'\") == 'Yes' else 0 for item in row]\n",
        "        processed_data.append(processed_row)\n",
        "\n",
        "    df_processed = pd.DataFrame(processed_data, columns=['Goal', 'Activity', 'Strategy', 'Plan', 'Structure', 'Innovation', 'Tactics', 'Relevance'])\n",
        "\n",
        "    # Reset the index of df_test and df_processed before concatenation\n",
        "    df_test.reset_index(drop=True, inplace=True)\n",
        "    df_processed.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    final_df = pd.concat([df_test['paragraph'], df_processed, df_test['processed_paragraph']], axis=1)\n",
        "\n",
        "    new_df_test = df_test.copy()\n",
        "    cols_to_convert = new_df_test.columns[~new_df_test.columns.isin(['processed_paragraph', 'paragraph'])]\n",
        "    new_df_test[cols_to_convert] = new_df_test[cols_to_convert].replace({'Yes': 1, 'No': 0})\n",
        "\n",
        "    columns = ['Goal', 'Activity', 'Strategy', 'Plan', 'Structure', 'Innovation', 'Tactics', 'Relevance']\n",
        "    # Renamed variables to avoid conflict\n",
        "    accuracy_scores = {}\n",
        "    f1_scores = {}\n",
        "    classification_reports = {}\n",
        "\n",
        "    for col in columns:\n",
        "        y_pred = final_df[col]\n",
        "        y_true = new_df_test[col]\n",
        "\n",
        "        # Use the renamed function references\n",
        "        accuracy = acc_score(y_true, y_pred)\n",
        "        f1_value = f1(y_true, y_pred)\n",
        "\n",
        "        accuracy_scores[col] = accuracy\n",
        "        f1_scores[col] = f1_value\n",
        "\n",
        "        report = class_rep(y_true, y_pred)\n",
        "        classification_reports[col] = report\n",
        "\n",
        "    return accuracy_scores, f1_scores, classification_reports\n"
      ],
      "metadata": {
        "id": "dLt5VWuVoYLt"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n"
      ],
      "metadata": {
        "id": "hNM3O3SrEGrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation data\n",
        "The accuracy scores, F1 scores and the classification reports for the validation dataset (out-sample dataset), are presented below."
      ],
      "metadata": {
        "id": "NmXCWu3nENwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score, f1_score, classification_report = evaluate_classification(df_test, predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WxaqaYiQ_-XP",
        "outputId": "b0cffdcc-0384-4ca3-cf16-038ffc98dffc"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print or use the collected metrics as needed\n",
        "print(\"Accuracy Scores:\", accuracy_score)\n",
        "print(\"F1 Scores:\", f1_score)\n",
        "print(\"Classification Reports:\", classification_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8zocWXljAHxk",
        "outputId": "ca708df1-39ac-4d69-8db6-5cd7c269f46e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Scores: {'Goal': 0.8333333333333334, 'Activity': 0.5833333333333334, 'Strategy': 0.75, 'Plan': 0.3333333333333333, 'Structure': 0.75, 'Innovation': 0.75, 'Tactics': 0.4166666666666667, 'Relevance': 0.16666666666666666}\n",
            "F1 Scores: {'Goal': 0.0, 'Activity': 0.6666666666666666, 'Strategy': 0.8, 'Plan': 0.0, 'Structure': 0.6666666666666666, 'Innovation': 0.7999999999999999, 'Tactics': 0.2222222222222222, 'Relevance': 0.16666666666666669}\n",
            "Classification Reports: {'Goal': '              precision    recall  f1-score   support\\n\\n           0       0.91      0.91      0.91        11\\n           1       0.00      0.00      0.00         1\\n\\n    accuracy                           0.83        12\\n   macro avg       0.45      0.45      0.45        12\\nweighted avg       0.83      0.83      0.83        12\\n', 'Activity': '              precision    recall  f1-score   support\\n\\n           0       0.50      0.40      0.44         5\\n           1       0.62      0.71      0.67         7\\n\\n    accuracy                           0.58        12\\n   macro avg       0.56      0.56      0.56        12\\nweighted avg       0.57      0.58      0.57        12\\n', 'Strategy': '              precision    recall  f1-score   support\\n\\n           0       1.00      0.50      0.67         6\\n           1       0.67      1.00      0.80         6\\n\\n    accuracy                           0.75        12\\n   macro avg       0.83      0.75      0.73        12\\nweighted avg       0.83      0.75      0.73        12\\n', 'Plan': '              precision    recall  f1-score   support\\n\\n           0       1.00      0.33      0.50        12\\n           1       0.00      0.00      0.00         0\\n\\n    accuracy                           0.33        12\\n   macro avg       0.50      0.17      0.25        12\\nweighted avg       1.00      0.33      0.50        12\\n', 'Structure': '              precision    recall  f1-score   support\\n\\n           0       1.00      0.67      0.80         9\\n           1       0.50      1.00      0.67         3\\n\\n    accuracy                           0.75        12\\n   macro avg       0.75      0.83      0.73        12\\nweighted avg       0.88      0.75      0.77        12\\n', 'Innovation': '              precision    recall  f1-score   support\\n\\n           0       0.60      0.75      0.67         4\\n           1       0.86      0.75      0.80         8\\n\\n    accuracy                           0.75        12\\n   macro avg       0.73      0.75      0.73        12\\nweighted avg       0.77      0.75      0.76        12\\n', 'Tactics': '              precision    recall  f1-score   support\\n\\n           0       0.36      1.00      0.53         4\\n           1       1.00      0.12      0.22         8\\n\\n    accuracy                           0.42        12\\n   macro avg       0.68      0.56      0.38        12\\nweighted avg       0.79      0.42      0.33        12\\n', 'Relevance': '              precision    recall  f1-score   support\\n\\n           0       0.10      0.50      0.17         2\\n           1       0.50      0.10      0.17        10\\n\\n    accuracy                           0.17        12\\n   macro avg       0.30      0.30      0.17        12\\nweighted avg       0.43      0.17      0.17        12\\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In-sample data\n",
        "The accuracy scores, F1 scores and the classification reports for the out-sample dataset are presented below."
      ],
      "metadata": {
        "id": "jer0ryrLEV1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_part = df_train.sample(n=10, random_state=42)"
      ],
      "metadata": {
        "id": "DG7Q68YzZc6F"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model_id = \"gpt-3.5-turbo\"\n",
        "predictions_insample = make_predictions(df_train_part, fine_tuned_model_id, client)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HR5IhAGIUK38",
        "outputId": "1347f890-329e-4af1-d8a7-2153e09ed0f8"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat completion succeeded for sentence 0  and batch  0\n",
            "Chat completion succeeded for sentence 1  and batch  0\n",
            "Chat completion succeeded for sentence 2  and batch  0\n",
            "Chat completion succeeded for sentence 3  and batch  0\n",
            "Chat completion succeeded for sentence 4  and batch  0\n",
            "Chat completion succeeded for sentence 5  and batch  0\n",
            "Chat completion succeeded for sentence 6  and batch  0\n",
            "Chat completion succeeded for sentence 7  and batch  0\n",
            "Chat completion succeeded for sentence 8  and batch  0\n",
            "Chat completion succeeded for sentence 9  and batch  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score_insample, f1_score_insample, classification_report_insample = evaluate_classification(df_train_part, predictions_insample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nQDjmG1lURvq",
        "outputId": "348fd2bf-67ad-4dae-f510-2e3e3131dd8f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print or use the collected metrics as needed\n",
        "print(\"Accuracy Scores for insample data:\", accuracy_score_insample)\n",
        "print(\"F1 Scores for insample data:\", f1_score_insample)\n",
        "print(\"Classification Reports for insample data:\", classification_report_insample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MQUfmP6_axm5",
        "outputId": "a423ee3c-abc6-41ae-a361-619bedf4f7e9"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Scores for insample data: {'Goal': 0.9, 'Activity': 0.8, 'Strategy': 0.8, 'Plan': 0.9, 'Structure': 0.9, 'Innovation': 0.9, 'Tactics': 0.8, 'Relevance': 0.3}\n",
            "F1 Scores for insample data: {'Goal': 0.6666666666666666, 'Activity': 0.75, 'Strategy': 0.5, 'Plan': 0.8, 'Structure': 0.8, 'Innovation': 0.6666666666666666, 'Tactics': 0.0, 'Relevance': 0.0}\n",
            "Classification Reports for insample data: {'Goal': '              precision    recall  f1-score   support\\n\\n           0       0.89      1.00      0.94         8\\n           1       1.00      0.50      0.67         2\\n\\n    accuracy                           0.90        10\\n   macro avg       0.94      0.75      0.80        10\\nweighted avg       0.91      0.90      0.89        10\\n', 'Activity': '              precision    recall  f1-score   support\\n\\n           0       0.83      0.83      0.83         6\\n           1       0.75      0.75      0.75         4\\n\\n    accuracy                           0.80        10\\n   macro avg       0.79      0.79      0.79        10\\nweighted avg       0.80      0.80      0.80        10\\n', 'Strategy': '              precision    recall  f1-score   support\\n\\n           0       1.00      0.78      0.88         9\\n           1       0.33      1.00      0.50         1\\n\\n    accuracy                           0.80        10\\n   macro avg       0.67      0.89      0.69        10\\nweighted avg       0.93      0.80      0.84        10\\n', 'Plan': '              precision    recall  f1-score   support\\n\\n           0       1.00      0.88      0.93         8\\n           1       0.67      1.00      0.80         2\\n\\n    accuracy                           0.90        10\\n   macro avg       0.83      0.94      0.87        10\\nweighted avg       0.93      0.90      0.91        10\\n', 'Structure': '              precision    recall  f1-score   support\\n\\n           0       0.88      1.00      0.93         7\\n           1       1.00      0.67      0.80         3\\n\\n    accuracy                           0.90        10\\n   macro avg       0.94      0.83      0.87        10\\nweighted avg       0.91      0.90      0.89        10\\n', 'Innovation': '              precision    recall  f1-score   support\\n\\n           0       1.00      0.89      0.94         9\\n           1       0.50      1.00      0.67         1\\n\\n    accuracy                           0.90        10\\n   macro avg       0.75      0.94      0.80        10\\nweighted avg       0.95      0.90      0.91        10\\n', 'Tactics': '              precision    recall  f1-score   support\\n\\n           0       0.80      1.00      0.89         8\\n           1       0.00      0.00      0.00         2\\n\\n    accuracy                           0.80        10\\n   macro avg       0.40      0.50      0.44        10\\nweighted avg       0.64      0.80      0.71        10\\n', 'Relevance': '              precision    recall  f1-score   support\\n\\n           0       0.30      1.00      0.46         3\\n           1       0.00      0.00      0.00         7\\n\\n    accuracy                           0.30        10\\n   macro avg       0.15      0.50      0.23        10\\nweighted avg       0.09      0.30      0.14        10\\n'}\n"
          ]
        }
      ]
    }
  ]
}